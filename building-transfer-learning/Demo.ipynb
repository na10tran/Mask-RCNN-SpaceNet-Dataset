{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IEhw9EHf9ZcnB8ZmJWE4q9TTaHVqdvyA","timestamp":1699783173230},{"file_id":"1-p5wOkdIDqKDphMoyJsl5X7OltxU6lnY","timestamp":1699575580192},{"file_id":"10cEc8zIt21ETx7BE4pWYPgNkZM3Vm4mF","timestamp":1660958034890},{"file_id":"1yNvJgswiFVWyMYfvqiLMlllm-BrAKUwl","timestamp":1598917120812}],"collapsed_sections":["KsNpU5N865gv","CZRP4X6E7Flg","qQmv4FYH8y0g","XZeRtae482ev"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u8b-OpFXR1Nv"},"source":["# CSC 480-F23 LandscAIpe Demo - Deep Learning Object Segmentation with Mask R-CNN\n","\n"]},{"cell_type":"markdown","source":["## Version history"],"metadata":{"id":"gMNa8-P9PqHM"}},{"cell_type":"markdown","source":["- Version 1: Fall 2023"],"metadata":{"id":"82cwSVmmPB5v"}},{"cell_type":"markdown","metadata":{"id":"BlvJxOkXQf0o"},"source":["# Authors:\n"]},{"cell_type":"markdown","metadata":{"id":"6y0M9Wq5S2Wv"},"source":["***Nathan Tran***\n","***Ethan Outangoun***\n","***Skyler Han***\n","***Saanvi Dua***\n","***Christine Widden***\n","\n","California Polytechnic State University, San Luis Obispo;\n","\n","Computer Science & Software Engineering Department"]},{"cell_type":"markdown","metadata":{"id":"pX-MlbrETMe-"},"source":["## Project Description"]},{"cell_type":"markdown","metadata":{"id":"uO-7j2l_R-9M"},"source":["This document explores the"]},{"cell_type":"markdown","metadata":{"id":"SxJieQY_lrls"},"source":["```\n","# Explain what this document is about, and what it does.\n","# Instead of \"Abstract\", you could also label it \"Project Description\", or leave out the label.\n","# After reading this, the potential user should be able to make a decision about engaging further with it, or go elsewhere.\n","# Point out the benefits and value for the prospective user.\n","# If applicable, identify the main technologies, tools, libraries, data sets, other resources that it will use.\n","# Identify the sources and creators of those resources, usually through links to their respective Web sites, or references to publications.\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"VyydW4wm_bue"},"source":["# Copyright: Apache 2.0 License\n"]},{"cell_type":"markdown","metadata":{"id":"XQNmPZ8LnjFU"},"source":["```\n","# By posting a notebook like this on the Web, you are making it publicly accessible, and it's hard to control who uses it how.\n","# To make this more explicit, I suggest to declare it as open source, and specify the license and main conditions under which it can be used.\n","# This won't prevent unauthorized use, but at least your intentions are clear.  \n","```\n"]},{"cell_type":"markdown","metadata":{"id":"wuYUD6DXQ_PD"},"source":["\n","```\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"az6mkV5OHQEa"},"source":["# Colab Basics\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pVI_bs4tQILD"},"source":["## Colab and Jupyter Notebooks\n"]},{"cell_type":"markdown","source":["### Setting up Conda Environment"],"metadata":{"id":"XIt6rUee7oXs"}},{"cell_type":"code","source":["!pip install -q condacolab\n","import condacolab\n","condacolab.install()"],"metadata":{"id":"FM_hjid27tWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import condacolab\n","condacolab.check()"],"metadata":{"id":"KEsyqOhc7uxl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting Up Your Drive"],"metadata":{"id":"UkDFOmfuBc4o"}},{"cell_type":"markdown","source":["First run the command below to link your google drive to this colab notebook"],"metadata":{"id":"0J834NMHBkAp"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1NXhZ4XLBr6w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699862929634,"user_tz":480,"elapsed":19051,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}},"outputId":"384f7d14-8cd5-424f-a849-efddac339d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Next, you are going to want to create a new folder in your drive called \"Mask_RCNN Demo Final\". After creating the folder, you will want to access the new folder you just created. Type in the following and run the cell"],"metadata":{"id":"gk4w7dOIByA5"}},{"cell_type":"code","metadata":{"id":"UH-Fh3SkL5qR"},"source":["%cd drive/My Drive/Mask_RCNN Demo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will clone the mask_rcnn repository from github into this new project folder. Type in the following and run the cell\n"],"metadata":{"id":"5y61FuV5CNjk"}},{"cell_type":"code","source":["! git clone https://github.com/ahmedfgad/Mask-RCNN-TF2.git"],"metadata":{"id":"k3uCV57eCXuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, you will want to rename the folder \"kangaro-transfer-learning\" as as \"building-transfer-learning\". After renaming that, move this google colab notebook into the folder."],"metadata":{"id":"xgsKewWMCgQD"}},{"cell_type":"markdown","source":["After moving the notebook, you will want to access the Mask-RCNN-TF2 folder in your drive. Run the following cell"],"metadata":{"id":"UzO_lUH7C-aS"}},{"cell_type":"code","source":["%cd Mask-RCNN-TF2"],"metadata":{"id":"pO3qVnlCDJJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will need to set up your environment now and install all of the needed dependencies. Run the following cell that will install of the requirements."],"metadata":{"id":"bsKJ0sPfDT4f"}},{"cell_type":"code","source":["! python -m pip install -r 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/requirements.txt'"],"metadata":{"id":"z6VrkxubDcIM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699874232267,"user_tz":480,"elapsed":202665,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}},"outputId":"5e91302b-b034-4465-f7c6-e89ee8f9846d"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.20.3\n","  Downloading numpy-1.20.3.zip (7.8 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/7.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting scipy==1.4.1\n","  Downloading scipy-1.4.1.tar.gz (24.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"markdown","source":["Now your set up is complete and you can move onto actually training your model!"],"metadata":{"id":"EI-U13v1Djig"}},{"cell_type":"markdown","source":[],"metadata":{"id":"DQUOcUl47xcL"}},{"cell_type":"code","source":["!conda install gdal\n","!apt-get install -y libgdal-dev\n","!conda install geoio"],"metadata":{"id":"IAXcujld7xvd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dm4rb6kITXQo"},"source":["# Overview"]},{"cell_type":"markdown","metadata":{"id":"r1BYWB1QTeue"},"source":["This section gives the participants a preview of the activity they are going to work on. It should be short, maybe one paragraph or so. If more information is needed, add extra sections below."]},{"cell_type":"markdown","metadata":{"id":"7mknHApYUOoz"},"source":["## Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"zRZebPe-UUYf"},"source":["Basic knowledge of linux commands and python is necessary. The tutorial is pretty simple and setting up the environment is mainly done through running the cells."]},{"cell_type":"markdown","metadata":{"id":"Ysx6aVbsUiLk"},"source":["##Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"nU2BOLDGUluv"},"source":["*   Understand a basic concept of mask_rcnn.\n","*   Be able to edit and train a machine learning model.\n","*   Mask an image with the trained model using testing images.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1--NMRPQpbFD"},"source":["# Data Set\n"]},{"cell_type":"markdown","source":["The dataset we will be using is from SpaceNet."],"metadata":{"id":"8d5JikX9EZ5K"}},{"cell_type":"markdown","metadata":{"id":"DtuIL90ABohd"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"gUDJlaYkKbDQ"},"source":["### Exploration of the Data"]},{"cell_type":"markdown","source":["The dataset is pretty simple. It is a bunch of sattelite images of homes. However, there is a decent amount of preprocessing we are going to have to do. For mask_rcnn to train, we are going need to have to create json tags for all of the images."],"metadata":{"id":"G4WmLDBpEh97"}},{"cell_type":"markdown","metadata":{"id":"UbTp2LMvE8sH"},"source":["# Preprocessing"]},{"cell_type":"markdown","source":["Mask R-CNN requires there to be labeled json files for each image and for the image to be ordered alphabetically for training. We will use methods developed by Mstfakts to preprocess all of our image files from SpaceNet-AI."],"metadata":{"id":"Ki4HvC4r9aXA"}},{"cell_type":"code","source":["#Access building-transfer-learning folder\n","%cd building-transfer-learning"],"metadata":{"id":"PtD5dJVW9aqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Renaming RGB-PanSharpen and GeoJson Files"],"metadata":{"id":"KsNpU5N865gv"}},{"cell_type":"markdown","source":["We will need to run the Rename_Files.ipynb. Without running this file, you will get errors because of the naming of the training files. Moreover, you will also need to order the training files by its name as they are unordered. Run the following cell to do this.\n"],"metadata":{"id":"xmM8cXhLE8sT"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import re\n","PATH = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/'\n","Geolar = os.listdir(PATH)\n","for i in range(len(Geolar)):\n","    oldNameGeo = os.path.join(PATH, Geolar[i])\n","    match = re.search(r'(\\d+)\\.tif$', Geolar[i])  # Extract digits before \".tif\" at the end\n","    if match:\n","        number = match.group(1)\n","        newNameGeo = os.path.join(PATH, 'tif' + number + '.tif')\n","        os.rename(oldNameGeo, newNameGeo)"],"metadata":{"id":"seTYqVsPFx7b","executionInfo":{"status":"ok","timestamp":1699875669844,"user_tz":480,"elapsed":206,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","PATH = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/geojson/buildings/'\n","Geolar = os.listdir(PATH)\n","for i in range(len(Geolar)):\n","    oldNameGeo = os.path.join(PATH, Geolar[i])\n","    match = re.search(r'(\\d+)\\.geojson$', Geolar[i])  # Extract digits before \".tif\" at the end\n","    if match:\n","        number = match.group(1)\n","        newNameGeo = os.path.join(PATH, 'geojson' + number + '.geojson')\n","        os.rename(oldNameGeo, newNameGeo)"],"metadata":{"id":"j-HW8evu6pEx","executionInfo":{"status":"ok","timestamp":1699876717804,"user_tz":480,"elapsed":555,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["### Convert RGB-PanSharpen Tif Files to PNG"],"metadata":{"id":"CZRP4X6E7Flg"}},{"cell_type":"markdown","source":["We have the dataset and its files' name are in order, but its format it TIFF. So, convert the TIFF file to the RGB file by running TIF_to_PNG.py. Run the following cell below to do this"],"metadata":{"id":"R18OddIXFyUN"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","from osgeo import gdal\n","import numpy as np\n","import os\n","import subprocess\n","\n","def bit16_to_8Bit(inputRaster, outputRaster, outputPixType='Byte', outputFormat='png', percentiles=[2, 98]):\n","    '''\n","    Convert 16bit image to 8bit\n","    Source: Medium.com, 'Creating Training Datasets for the SpaceNet Road Detection and Routing Challenge' by Adam Van Etten and Jake Shermeyer\n","    '''\n","\n","    srcRaster = gdal.Open(inputRaster)\n","    cmd = ['gdal_translate', '-ot', outputPixType, '-of',\n","           outputFormat]\n","\n","    # iterate through bands\n","    for bandId in range(srcRaster.RasterCount):\n","        bandId = bandId+1\n","        band = srcRaster.GetRasterBand(bandId)\n","\n","        bmin = band.GetMinimum()\n","        bmax = band.GetMaximum()\n","        # if not exist minimum and maximum values\n","        if bmin is None or bmax is None:\n","            (bmin, bmax) = band.ComputeRasterMinMax(1)\n","        # else, rescale\n","        band_arr_tmp = band.ReadAsArray()\n","        bmin = np.percentile(band_arr_tmp.flatten(),\n","                             percentiles[0])\n","        bmax= np.percentile(band_arr_tmp.flatten(),\n","                            percentiles[1])\n","\n","        cmd.append('-scale_{}'.format(bandId))\n","        cmd.append('{}'.format(bmin))\n","        cmd.append('{}'.format(bmax))\n","        cmd.append('{}'.format(0))\n","        cmd.append('{}'.format(255))\n","    cmd.append(inputRaster)\n","    cmd.append(outputRaster)\n","    print(\"Conversin command:\", cmd)\n","    subprocess.call(cmd)\n","\n","path = \"drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/\"\n","files = os.listdir(path)\n","\n","for file in files:\n","    resimPath = path+file\n","    dstPath   = \"drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/\"\n","    dstPath   = dstPath+file\n","    bit16_to_8Bit(resimPath,dstPath)"],"metadata":{"id":"p5mrEqM-FzLw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699876020390,"user_tz":480,"elapsed":11301,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}},"outputId":"dc547232-1b57-4c2a-c924-f9b080d64521"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1275.0', '0', '255', '-scale_2', '0.0', '1772.0', '0', '255', '-scale_3', '0.0', '1249.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image36.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image36.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '677.0', '0', '255', '-scale_2', '0.0', '857.0', '0', '255', '-scale_3', '0.0', '600.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image35.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image35.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '640.0', '0', '255', '-scale_2', '0.0', '776.0', '0', '255', '-scale_3', '0.0', '549.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image32.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image32.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '748.0200000000186', '0', '255', '-scale_2', '0.0', '959.0', '0', '255', '-scale_3', '0.0', '656.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image30.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image30.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '951.0', '0', '255', '-scale_2', '0.0', '966.0', '0', '255', '-scale_3', '0.0', '656.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image3.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image3.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '696.0', '0', '255', '-scale_2', '0.0', '842.0', '0', '255', '-scale_3', '0.0', '591.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image29.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image29.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '712.0', '0', '255', '-scale_2', '0.0', '874.0', '0', '255', '-scale_3', '0.0', '601.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image28.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image28.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '692.0', '0', '255', '-scale_2', '0.0', '909.0', '0', '255', '-scale_3', '0.0', '635.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image25.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image25.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '758.0', '0', '255', '-scale_2', '0.0', '959.0', '0', '255', '-scale_3', '0.0', '649.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image23.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image23.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '835.0', '0', '255', '-scale_2', '0.0', '1075.0', '0', '255', '-scale_3', '0.0', '737.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image22.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image22.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '722.0', '0', '255', '-scale_2', '0.0', '990.0', '0', '255', '-scale_3', '0.0', '713.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image18.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image18.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1006.0', '0', '255', '-scale_2', '0.0', '1139.0', '0', '255', '-scale_3', '0.0', '752.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image14.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image14.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1044.0', '0', '255', '-scale_2', '0.0', '1168.0', '0', '255', '-scale_3', '0.0', '763.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image13.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image13.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1013.0', '0', '255', '-scale_2', '0.0', '1377.0', '0', '255', '-scale_3', '0.0', '915.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image12.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image12.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1001.0', '0', '255', '-scale_2', '0.0', '1347.0', '0', '255', '-scale_3', '0.0', '926.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image10.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image10.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '807.0', '0', '255', '-scale_2', '0.0', '890.0', '0', '255', '-scale_3', '0.0', '610.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image1.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image1.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '937.0', '0', '255', '-scale_2', '0.0', '1136.0', '0', '255', '-scale_3', '0.0', '760.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image9.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image9.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '895.0', '0', '255', '-scale_2', '0.0', '1062.0', '0', '255', '-scale_3', '0.0', '705.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image8.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image8.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '947.0', '0', '255', '-scale_2', '0.0', '1101.0', '0', '255', '-scale_3', '0.0', '727.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image7.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image7.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '946.0', '0', '255', '-scale_2', '0.0', '1080.0', '0', '255', '-scale_3', '0.0', '725.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image50.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image50.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '875.0', '0', '255', '-scale_2', '0.0', '967.0', '0', '255', '-scale_3', '0.0', '659.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image47.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image47.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '727.0', '0', '255', '-scale_2', '0.0', '811.0', '0', '255', '-scale_3', '0.0', '573.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image48.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image48.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1124.0200000000186', '0', '255', '-scale_2', '0.0', '1256.0', '0', '255', '-scale_3', '0.0', '803.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image45.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image45.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '1158.0', '0', '255', '-scale_2', '0.0', '1287.0', '0', '255', '-scale_3', '0.0', '819.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image44.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image44.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '937.0', '0', '255', '-scale_2', '0.0', '1104.0', '0', '255', '-scale_3', '0.0', '741.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image42.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image42.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '988.0', '0', '255', '-scale_2', '0.0', '1070.0', '0', '255', '-scale_3', '0.0', '694.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image40.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image40.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '939.0', '0', '255', '-scale_2', '0.0', '934.0', '0', '255', '-scale_3', '0.0', '630.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image4.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image4.tif']\n","Conversin command: ['gdal_translate', '-ot', 'Byte', '-of', 'png', '-scale_1', '0.0', '736.0', '0', '255', '-scale_2', '0.0', '971.0', '0', '255', '-scale_3', '0.0', '693.0', '0', '255', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/image37.tif', 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image37.tif']\n"]}]},{"cell_type":"markdown","source":["### Create Labels for TIFF and GeoJson Files"],"metadata":{"id":"2mM7KHh47QeE"}},{"cell_type":"markdown","source":["To train the model, we need the labels, too. We will create these labels by using TIFF files and its corresponding GeoJSON files. We need the TIFF file to adjust the position of the GeoJSON coordinates to the specific picture. You only need to follow Create_Masks.ipynb"],"metadata":{"id":"PZCQk8EkFzif"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import json\n","import geoio"],"metadata":{"id":"95qnCtseF6AL","executionInfo":{"status":"ok","timestamp":1699876124864,"user_tz":480,"elapsed":274,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["ResimPATH = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen/'\n","# All the pictures from the given path\n","ResimAdlari = os.listdir(ResimPATH)\n","GeoJSONPATH = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/geojson/buildings/'\n","# All the geojson-files from the given path\n","GeoAdlari = os.listdir(GeoJSONPATH)"],"metadata":{"id":"3pMC5zd08hsf","executionInfo":{"status":"ok","timestamp":1699876795948,"user_tz":480,"elapsed":244,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# DosyaNumarasi takes the number from 0 to how big your training dataset\n","for DosyaNumarasi in range(len(GeoAdlari)):\n","\n","    # data holds the info for the each GeoJSON-file\n","    with open(GeoJSONPATH+'/'+GeoAdlari[DosyaNumarasi]) as f:\n","        data = json.load(f)\n","\n","    # RGBTIFResmi holds the info for each TIF-file\n","    RGBTIFResmi = geoio.GeoImage(ResimPATH+'/'+ResimAdlari[DosyaNumarasi])\n","\n","    cokgenler = [] # Hold the coordinates for each building in the picture.\n","                   # (Outside for loop is for each picture, and here, cokgenler\n","                   # will hold the coordinates for each building in one picture.)\n","    types = [] # Holds the type of the buldings (MultiPolygon - Partial Building - Point)\n","               # We are not interested in the points.\n","\n","    # Create the pane size of 650x650 to put the figures from geojson-file. Otherwise,\n","    # the buildings may be flipped or they may saved one by one\n","    arkaPlan = np.zeros([650,650])\n","    plt.imshow(arkaPlan)\n","\n","    try:\n","        # We do not know how many buildings the picture includes.\n","        # So, we just give very big number to make sure that we utilized\n","        # all the buildings in one picture.\n","        # In short, bina keeps what order the building is.\n","        for bina in range(2000):\n","            tip = str(data['features'][bina]['geometry']['type'])\n","            types.append(tip) # Append all the type of the buildings\n","\n","            # If type is point, do not do anything\n","            if tip == ('Point'):\n","                pass\n","\n","            # If type is MultiPolygon, cokgenler will hold the coordinates\n","            elif tip == ('MultiPolygon'):\n","                kucukBinalar = (data['features'][bina]['geometry']['coordinates'])\n","                for b in range(len(kucukBinalar)):\n","                    cokgenler.append(kucukBinalar[b])\n","\n","            # For the rest of the types, cokgenler will hold the coordinates again\n","            else:\n","                cokgenler.append(data['features'][bina]['geometry']['coordinates'])\n","\n","    except IndexError:\n","        # If we utilized all the buildings in the given picture,\n","        # lest create mask for each one.\n","\n","        # cokgenBina holds the each building's coordinates\n","        for cokgenBina in cokgenler:\n","\n","            # binaNoktalari holds the individual edge coordinates for each building.\n","            for binaNoktalari in cokgenBina:\n","\n","                # To hold the edge coordinates (in pixel form)\n","                doldurX = []\n","                doldurY = []\n","\n","                # noktas holds x and y for each edge coordinate\n","                for noktas in binaNoktalari:\n","\n","                    # Convert Latitude&Longitude to the pixels\n","                    xPixel, yPixel = RGBTIFResmi.proj_to_raster(noktas[0], noktas[1])\n","\n","                    # The pixels may be 650 which defaces the masks.\n","                    xPixel = 649 if xPixel > 649 else xPixel\n","                    yPixel = 649 if yPixel > 649 else yPixel\n","\n","                    # Keep x and y in pixel form\n","                    doldurX.append(xPixel)\n","                    doldurY.append(yPixel)\n","\n","                # To paint between given pixel values\n","                plt.fill_between(doldurX, doldurY, facecolor='red')\n","\n","                # To remove white area around matplotlib figure\n","                fig = plt.figure(1)\n","                extent = plt.gca().get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n","\n","                # Adjust the DPI for 650x650\n","                # and save the figure\n","                # While saving, you should put them in order; 0 to ...\n","                fig.savefig('drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/MASKS/'+str(DosyaNumarasi)+'.png', bbox_inches=extent, dpi=215.24)\n","\n","        # Close the figure after an image is done.\n","        plt.close()"],"metadata":{"id":"YoReXNIF8kRV","executionInfo":{"status":"ok","timestamp":1699876932636,"user_tz":480,"elapsed":134919,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["### Display GeoJSON"],"metadata":{"id":"qQmv4FYH8y0g"}},{"cell_type":"code","source":["import geopandas as gpd #To read geojson file\n","import matplotlib.pyplot as plt #To display the geojson file\n","%matplotlib inline"],"metadata":{"id":"92vPURfv88A7","executionInfo":{"status":"ok","timestamp":1699876937161,"user_tz":480,"elapsed":175,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["geoFileInfo = gpd.read_file('drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/geojson/buildings/image_geojson1.geojson')"],"metadata":{"id":"Pc4jKQ8r9CLO","executionInfo":{"status":"ok","timestamp":1699876989523,"user_tz":480,"elapsed":163,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["geoFileInfo #Exhibit the values"],"metadata":{"id":"TkFSyG9V9D37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["geoFileInfo.plot(color='black') # Plot the file"],"metadata":{"id":"FudLwwrK9Gfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Display Mask and RGB Images"],"metadata":{"id":"XZeRtae482ev"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image"],"metadata":{"id":"_LyO7pyS9K3x","executionInfo":{"status":"ok","timestamp":1699867275556,"user_tz":480,"elapsed":197,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["image_id = 1 # Give the image ID"],"metadata":{"id":"wTJyZrZh9NJI","executionInfo":{"status":"ok","timestamp":1699877388285,"user_tz":480,"elapsed":267,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["ResimAdi = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/RGB-PanSharpen-NEW/image'+str(image_id)+'.tif'\n","imggg = Image.open(ResimAdi).convert(\"L\")\n","numpyResim = np.array(imggg)\n","imggg.close()\n","\n","ResimPATH = 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/MASKS/'+str(image_id)+'.png'\n","imggg = Image.open(ResimPATH)\n","imgarr = np.array(imggg)\n","imggg.close()\n","\n","fig = plt.figure(1, figsize = [20, 10], dpi = 200)\n","plt.subplot(1,2,1)\n","plt.imshow(imgarr)\n","plt.subplot(1,2,2)\n","plt.imshow(numpyResim)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"WugaFETP9OqY","executionInfo":{"status":"error","timestamp":1699877389358,"user_tz":480,"elapsed":16,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}},"outputId":"e686fe20-bc56-4427-8d9f-dc059ec203f3"},"execution_count":69,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-3144537e8643>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mResimPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/MASKS/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimggg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResimPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mimgarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimggg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimggg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/MASKS/1.png'"]}]},{"cell_type":"markdown","metadata":{"id":"5oNNvkgvpgXa"},"source":["# Model"]},{"cell_type":"markdown","source":["This term is often used in connection with Machine Learning tools and refers to a specific configuration of a network (e.g., ResNet or YOLOv7). For other tools, you can describe the system architecture in terms of the main components it uses, and how they interact."],"metadata":{"id":"4ikgpcsFOPDI"}},{"cell_type":"markdown","metadata":{"id":"k6Ksvji1MOZo"},"source":["## Model Selection"]},{"cell_type":"markdown","metadata":{"id":"F93VhhaeMiuO"},"source":["## Model Design"]},{"cell_type":"markdown","metadata":{"id":"yt-lnkh-MpgR"},"source":["Most often, the model will be based on an existing architecture, and built from components of a framework like PyTorch or TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"7wmXO7-cpvv1"},"source":["## Model Training"]},{"cell_type":"markdown","source":["To train the model, run the following cell"],"metadata":{"id":"MepjVKXCKjfa"}},{"cell_type":"code","source":["import os\n","import sys\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw\n","\n","#Libraries' order is important\n","import geopandas as gpd\n","# Geoio dan önce geopandası yüklemelisin.\n","import geoio\n","import json\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/\")\n","DATASET_DIR = os.path.abspath(\"drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/building-transfer-learning/building/\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn.config import Config\n","from mrcnn import utils\n","import mrcnn.model as modellib\n","from mrcnn import visualize\n","from mrcnn.model import log\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"HDusqzkWWd_Y","executionInfo":{"status":"error","timestamp":1699877308974,"user_tz":480,"elapsed":210,"user":{"displayName":"Nathan Tran","userId":"15148529163724474161"}},"outputId":"d7fe7234-187f-41de-f3fa-13d44253d28a"},"execution_count":65,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-89fd3bac4a51>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Mask_RCNN Demo/Mask-RCNN-TF2/mrcnn/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["class SpaceNetConfig(Config):\n","    \"\"\"Configuration for training on the toy shapes dataset.\n","    Derives from the base Config class and overrides values specific\n","    to the toy shapes dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"SpaceNet\"\n","    BACKBONE = \"resnet50\"\n","    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n","    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 1  # background + 1 building\n","\n","    # Use small images for faster training. Set the limits of the small side\n","    # the large side, and that determines the image shape.\n","    IMAGE_MIN_DIM = 640\n","    IMAGE_MAX_DIM = 640\n","\n","    # Use smaller anchors because our image and objects are small\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n","    RPN_ANCHOR_RATIOS = [0.25, 1, 4]\n","\n","    # Reduce training ROIs per image because the images are small and have\n","    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n","    TRAIN_ROIS_PER_IMAGE = 32\n","\n","    USE_MINI_MASK = True\n","\n","    # Use a small epoch since the data is simple\n","    STEPS_PER_EPOCH = 500\n","\n","    # use small validation steps since the epoch is small\n","    VALIDATION_STEPS = 50\n","\n","    MAX_GT_INSTANCES=250\n","    DETECTION_MAX_INSTANCES=350\n","\n","config = SpaceNetConfig()\n","config.display()\n","\n","def get_ax(rows=1, cols=1, size=8):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","\n","    Change the default size attribute to control the size\n","    of rendered images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax\n","\n","def fill_between(polygon):\n","    \"\"\"\n","    Returns: a bool array\n","    \"\"\"\n","    img = Image.new('1', (650, 650), False)\n","    ImageDraw.Draw(img).polygon(polygon, outline=True, fill=True)\n","    mask = np.array(img)\n","    return mask\n","\n","class SpaceNetDataset(utils.Dataset):\n","    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n","    shapes (triangles, squares, circles) placed randomly on a blank surface.\n","    The images are generated on the fly. No file access required.\n","    \"\"\"\n","\n","    def load_dataset(self, dataset_dir, start=1, end=400):\n","        \"\"\"Generate the requested number of synthetic images.\n","        count: number of images to generate.\n","        height, width: the size of the generated images.\n","        \"\"\"\n","        # Add classes\n","        self.add_class(\"SpaceNetDataset\", 1, \"building\")\n","\n","        # define data locations for images and annotations\n","        images_dir = os.path.join(dataset_dir, \"RGB-PanSharpen1/\")\n","        annotations_dir = os.path.join(dataset_dir, \"geojson/buildings/\")\n","\n","        # Iterate through all files in the folder to\n","        #add class, images and annotaions\n","        for filename in os.listdir(images_dir)[start:end]:\n","            image_id  = filename[31:-4]\n","            image_dir = os.path.join(images_dir,str(filename))\n","            ann_path  = os.path.join(annotations_dir,\"buildings_AOI_2_Vegas_imgg\"+str(image_id)+\".geojson\")\n","            self.add_image('SpaceNetDataset', image_id=image_id, path=image_dir, annotation=ann_path)\n","\n","\n","    def load_image(self, image_id):\n","        \"\"\"Generate an image from the specs of the given image ID.\n","        Typically this function loads the image from a file, but\n","        in this case it generates the image on the fly from the\n","        specs in image_info.\n","        \"\"\"\n","        image_dir = os.path.join(DATASET_DIR, \"RGB-PanSharpen1/RGB-PanSharpen_AOI_2_Vegas_imgg\"+str(image_id)+\".png\")\n","        im = Image.open(image_dir)\n","        return np.asarray(im)\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the shapes data of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"shapes\":\n","            return info[\"shapes\"]\n","        else:\n","            super(self.__class__).image_reference(self, image_id)\n","\n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for shapes of the given image ID.\n","        \"\"\"\n","        masks = np.zeros((650,650))\n","        ResimPATH = 'D:/DATASET/SpaceNet/Train/AOI_2_Vegas_Train/RGB-PanSharpen/RGB-PanSharpen_AOI_2_Vegas_imgg'+str(image_id)+'.tif'\n","        RGBTIFResmi = geoio.GeoImage(ResimPATH)\n","\n","        with open(DATASET_DIR+\"/geojson/buildings/buildings_AOI_2_Vegas_imgg\"+str(image_id)+\".geojson\") as f:\n","            data = json.load(f)\n","            allBuildings = data['features']\n","\n","            for building in allBuildings:\n","                veri = building['geometry']['coordinates'][0]\n","\n","                tip = str(building['geometry']['type'])\n","                coordinates = list()\n","                if tip == ('Point'):\n","                    continue\n","\n","                elif tip == ('MultiPolygon'):\n","\n","                    if isinstance(veri,float): continue\n","\n","                    kucukBinalar = (building['geometry']['coordinates'])\n","                    for b in range(len(kucukBinalar)):\n","                        veri = kucukBinalar[b][0]\n","                        for i in veri:\n","                            xPixel, yPixel = RGBTIFResmi.proj_to_raster(i[0], i[1])\n","                            xPixel = 649 if xPixel > 649 else xPixel\n","                            yPixel = 649 if yPixel > 649 else yPixel\n","                            coordinates.append((xPixel,yPixel))\n","                else:\n","                    if isinstance(veri,float): continue\n","\n","                    for i in veri:\n","                        xPixel, yPixel = RGBTIFResmi.proj_to_raster(i[0], i[1])\n","                        xPixel = 649 if xPixel > 649 else xPixel\n","                        yPixel = 649 if yPixel > 649 else yPixel\n","                        coordinates.append((xPixel,yPixel))\n","\n","                maske = fill_between(coordinates)\n","                masks = np.dstack((masks,maske))\n","\n","        if masks.shape != (650,650):\n","            masks = masks[:,:,1:]\n","            class_ids = np.asarray([1]*masks.shape[2])\n","        else:\n","            class_ids=np.ones((1))\n","            masks = masks.reshape((650,650,1))\n","        return masks.astype(np.bool), class_ids.astype(np.int32)\n","\n","# Training dataset\n","dataset_train = SpaceNetDataset()\n","dataset_train.load_dataset(DATASET_DIR,0,3080)\n","dataset_train.prepare()\n","\n","# Validation dataset\n","dataset_val = SpaceNetDataset()\n","dataset_val.load_dataset(DATASET_DIR,3081,3850)\n","dataset_val.prepare()\n","\n","\"\"\"\n","# Load and display random samples\n","image_ids = np.random.choice(dataset_train.image_ids,4)\n","\n","for image_id in image_ids:\n","    print(image_id)\n","    image = dataset_train.load_image(image_id)\n","    mask, class_ids = dataset_train.load_mask(image_id)\n","    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n","\"\"\"\n","\"\"\"\n","# Create model in training mode\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\n","                          model_dir=MODEL_DIR)\n","\n","# Which weights to start with?\n","init_with = \"last\"  # imagenet, coco, or last\n","\n","if init_with == \"imagenet\":\n","    model.load_weights(model.get_imagenet_weights(), by_name=True)\n","elif init_with == \"coco\":\n","    # Load weights trained on MS COCO, but skip layers that\n","    # are different due to the different number of classes\n","    # See README for instructions to download the COCO weights\n","    model.load_weights(COCO_MODEL_PATH, by_name=True,\n","                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n","elif init_with == \"last\":\n","    # Load the last model you trained and continue training\n","    #model.load_weights(model.find_last(), by_name=True)\n","    model.load_weights(model.find_last(), by_name=True)\n","\n","\n","# Train the head branches\n","# Passing layers=\"heads\" freezes all layers except the head\n","# layers. You can also pass a regular expression to select\n","# which layers to train by name pattern.\n","model.train(dataset_train, dataset_val,\n","            learning_rate=config.LEARNING_RATE,\n","            epochs=200,\n","            layers='all')\n","\"\"\"\n","#########################################################DETECTION\n","\n","class InferenceConfig(SpaceNetConfig):\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","inference_config = InferenceConfig()\n","\n","# Recreate the model in inference mode\n","model = modellib.MaskRCNN(mode=\"inference\",\n","                          config=inference_config,\n","                          model_dir=MODEL_DIR)\n","\n","# Get path to saved weights\n","# Either set a specific path or find last trained weights\n","# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n","model_path = model.find_last()\n","\n","# Load trained weights\n","print(\"Loading weights from \", model_path)\n","model.load_weights(model_path, by_name=True)\n","\n","# Test on a random image\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config,\n","                           image_id, use_mini_mask=False)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n","                            dataset_train.class_names, figsize=(8, 8))\n","\n","results = model.detect([original_image], verbose=1)\n","r = results[0]\n","visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n","                            dataset_val.class_names, r['scores'], ax=get_ax())\n","\n","\n","\"\"\"\n","# Compute VOC-Style mAP @ IoU=0.5\n","# Running on 10 images. Increase for better accuracy.\n","image_ids = np.random.choice(dataset_val.image_ids, 6)\n","APs = []\n","for image_id in image_ids:\n","    # Load image and ground truth data\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","        modellib.load_image_gt(dataset_val, inference_config,\n","                               image_id, use_mini_mask=False)\n","    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n","    # Run object detection\n","    results = model.detect([image], verbose=0)\n","    r = results[0]\n","    # Compute AP\n","    AP, precisions, recalls, overlaps =\\\n","        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","    APs.append(AP)\n","\n","print(\"mAP: \", np.mean(APs))\n","\"\"\""],"metadata":{"id":"Zcmq8QpmKmUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZwp7BZcp0ny"},"source":["## Model Validation"]},{"cell_type":"markdown","source":["To validate the model, run the following cell"],"metadata":{"id":"Z8iLXH8DLEJc"}},{"cell_type":"code","source":["prediction_file_path = 'drive/My Drive/project_folder/Mask-RCNN-TF2/building-transfer-learning/kangaroo_prediction.py'\n","%run {prediction_file_path}"],"metadata":{"id":"DruCO6RhLG2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYs50RBkp9Gn"},"source":["# Conclusions"]},{"cell_type":"markdown","metadata":{"id":"HJMhDSlpqACD"},"source":["#Future Work"]},{"cell_type":"markdown","metadata":{"id":"pHHRcikTqGVz"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"dayCr5qlLIcD"},"source":["* Bonner, A. (2019). Getting Started With Google Colab. Medium.\n","https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c\n","* Dair.AI. (2019). Writing Primer for Data Scientists. DAIR Diverse Artificial Intelligence Research Initiative. https://colab.research.google.com/drive/1qi8bXjH389MipsFx3KVQQwGwefDgzzoe\n","* Elvis. (2018). Primer for Learning Google Colab. Medium. https://medium.com/dair-ai/primer-for-learning-google-colab-bb4cabca5dd6\n","* Google CoLab. (2018). Google Colaboratory: Welcome: Introduction to Machine Learning Labs. Google. https://colab.research.google.com/drive/1EIB6pAaMM6C2MIdxsrGntYrQoUfcsbmS#forceEdit=true&sandboxMode=true\n","* Sagar, A. (2019). One-Stop Guide to Google Colab. Medium. https://towardsdatascience.com/one-stop-guide-to-google-colab-d67c94d30516\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LO2KC-aBqKTU"},"source":["# Appendices"]},{"cell_type":"markdown","source":["If you have additional documents that don't fit well directly into the tutorial structure above, you can include links here. This could be more detailed technical documents about your projects, or descriptions of the data set."],"metadata":{"id":"nrCQyqiUeNTa"}},{"cell_type":"markdown","metadata":{"id":"GFEF9ZqKMAPQ"},"source":["# Acknowledgements\n","\n","\n","\n"]},{"cell_type":"markdown","source":["<!-- If you're using code, data sets, examples, images or other material based on other people's work, acknowledge it here. This includes parts created with the help of generative AI tools, such as ChatGPT, Google Bard, or Microsoft Copilot. For images and similar material, you should also include a reference anchor or a link to the original.  -->"],"metadata":{"id":"-FGWxtuFduuo"}}]}